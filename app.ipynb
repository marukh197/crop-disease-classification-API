{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1ffdd-f4c7-4e10-8d85-4d112b5acc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from flask import Flask, request, render_template, redirect, url_for, jsonify,session,flash\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from werkzeug.utils import secure_filename \n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from utils.add_class_to_dataset import add_class_to_dataset\n",
    "from utils.augment_image import augment_image\n",
    "from utils.generate_class_distribution_plot import generate_class_distribution_plot\n",
    "from utils.get_class_distribution import get_class_distribution\n",
    "from utils.remove_class_from_dataset import remove_class_from_dataset\n",
    "from utils.resample_classes import resample_classes\n",
    "from utils.split_dataset_by_ratio import split_dataset_by_ratio\n",
    "from utils.focal_loss import focal_loss\n",
    "from utils.create_model import create_model\n",
    "from utils.prepare_image import prepare_image\n",
    "from utils.get_available_models import get_available_models\n",
    "from utils.load_model_info import load_model_info\n",
    "from utils.merge_back_data_if_split import merge_back_data_if_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='app.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set image parameters\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32\n",
    "\n",
    "# Custom loss function\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def focal_loss_fixed(gamma=2., alpha=0.25):\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        alpha_t = y_true * alpha + (tf.keras.backend.ones(tf.keras.backend.shape(y_true)) - y_true) * (1 - alpha)\n",
    "        p_t = y_true * y_pred + (tf.keras.backend.ones(tf.keras.backend.shape(y_true)) - y_true) * (1 - y_pred)\n",
    "        fl = - alpha_t * tf.keras.backend.pow((tf.keras.backend.ones(tf.keras.backend.shape(y_true)) - p_t), gamma) * tf.keras.backend.log(p_t)\n",
    "        return tf.keras.backend.mean(fl)\n",
    "    return focal_loss\n",
    "\n",
    "# Define custom objects dictionary\n",
    "custom_objects = {\n",
    "    'focal_loss_fixed': focal_loss_fixed\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')  # Render the main index page with navigation links\n",
    "    \n",
    "\n",
    "@app.route('/dataset', methods=['GET', 'POST'])\n",
    "def dataset():\n",
    "    # Initialize variables to hold the response data\n",
    "    class_distribution = None\n",
    "    message = None\n",
    "    plot_url = None\n",
    "    error = None\n",
    "    dataset_path = None\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        # Check which form was submitted\n",
    "        if 'set_dataset' in request.form:\n",
    "            # Handle dataset path form\n",
    "            dataset_path = request.form.get('dataset_path', '').strip()\n",
    "\n",
    "            # Check if the dataset path exists\n",
    "            if not os.path.exists(dataset_path):\n",
    "                error = \"Dataset path does not exist.\"\n",
    "            else:\n",
    "                message = \"Dataset path set successfully!\"\n",
    "                class_distribution = get_class_distribution(dataset_path)\n",
    "                plot_url = generate_class_distribution_plot(class_distribution)\n",
    "\n",
    "        elif 'perform_operation' in request.form:\n",
    "            # Handle operations form\n",
    "            dataset_path = request.form.get('dataset_path_hidden')\n",
    "            if not dataset_path:\n",
    "                error = \"Please set the dataset path first.\"\n",
    "            else:\n",
    "                try:\n",
    "                    # Handle dataset operations based on user choice\n",
    "                    operation = request.form.get('operation')\n",
    "\n",
    "                    if operation == 'add_class':\n",
    "                        # Add a new class to the dataset\n",
    "                        new_class = request.form.get('new_class', '').strip()\n",
    "                        class_images_dir = request.form.get('class_images_dir', '').strip()\n",
    "                        if not new_class or not class_images_dir:\n",
    "                            error = \"Both class name and class images directory are required.\"\n",
    "                        else:\n",
    "                            message = add_class_to_dataset(dataset_path, new_class, class_images_dir)\n",
    "                            class_distribution = get_class_distribution(dataset_path)\n",
    "                            plot_url = generate_class_distribution_plot(class_distribution)\n",
    "\n",
    "                    elif operation == 'remove_class':\n",
    "                        # Remove selected class from the dataset\n",
    "                        class_to_remove = request.form.get('class_to_remove', '').strip()\n",
    "                        if not class_to_remove:\n",
    "                            error = \"Class to remove is required.\"\n",
    "                        else:\n",
    "                            message = remove_class_from_dataset(dataset_path, class_to_remove)\n",
    "                            class_distribution = get_class_distribution(dataset_path)\n",
    "                            plot_url = generate_class_distribution_plot(class_distribution)\n",
    "\n",
    "                    elif operation == 'resample':\n",
    "                        # Resample classes in the dataset\n",
    "                        target_count_str = request.form.get('target_count')\n",
    "                        if not target_count_str:\n",
    "                            error = \"Target count is required for resampling.\"\n",
    "                        else:\n",
    "                            target_count = int(target_count_str)\n",
    "                            resample_classes(dataset_path, target_count)\n",
    "                            message = \"Resampling done!\"\n",
    "                            class_distribution = get_class_distribution(dataset_path)\n",
    "                            plot_url = generate_class_distribution_plot(class_distribution)\n",
    "\n",
    "                    elif operation == 'split':\n",
    "                        # Check if data is already split into train/validation/test\n",
    "                        if any(os.path.exists(os.path.join(dataset_path, split)) for split in ['train', 'validation', 'test']):\n",
    "                            user_response = request.form.get('merge_back', '').lower()\n",
    "                            if user_response == 'yes':\n",
    "                                # Merge data back\n",
    "                                merge_back_data_if_split(dataset_path)\n",
    "                                message = \"Data merged back successfully. Ready to re-split.\"\n",
    "\n",
    "                        # Split the dataset into train/validation/test sets\n",
    "                        train_ratio = float(request.form.get('train_ratio', 0))\n",
    "                        val_ratio = float(request.form.get('validation_ratio', 0))\n",
    "                        test_ratio = float(request.form.get('test_ratio', 0))\n",
    "                        if train_ratio + val_ratio + test_ratio > 1:\n",
    "                            error = \"The sum of training, validation, and test ratios must not exceed 1.\"\n",
    "                        else:\n",
    "                            split_dataset_by_ratio(dataset_path, train_ratio, val_ratio, test_ratio)\n",
    "                            message = \"Dataset splitting done!\"\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"An error occurred: {e}\")\n",
    "                    error = str(e)\n",
    "\n",
    "    # Render the dataset management page for GET requests or if errors occurred\n",
    "    return render_template('dataset.html', message=message, error=error, class_distribution=class_distribution, plot_url=plot_url, dataset_path=dataset_path)\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/train', methods=['GET', 'POST'])\n",
    "def train():\n",
    "    error = None  # Initialize the error variable\n",
    "    message = None  # Initialize the message variable\n",
    "    training_logs = []  # Initialize logs\n",
    "    \n",
    "    if request.method == 'POST':  # Handle form submission\n",
    "        # Get the dataset path from the form input\n",
    "        dataset_path = request.form.get('dataset_path', '').strip()\n",
    "        \n",
    "        # Check if the dataset path exists\n",
    "        if not os.path.exists(dataset_path):\n",
    "            error = \"Dataset path does not exist.\"\n",
    "            return jsonify({'error': error, 'message': message, 'training_logs': training_logs})\n",
    "\n",
    "        base_model_name = request.form.get('base_model')\n",
    "        epochs = request.form.get('epochs')\n",
    "        fine_tune = request.form.get('fine_tune')\n",
    "        fine_tune_epochs = request.form.get('fine_tune_epochs')\n",
    "        batch_size = request.form.get('batch_size')  # Get batch size from form input\n",
    "        model_type = request.form.get('model_type')  # New input for model type (pest or crop)\n",
    "\n",
    "        # Validate the input\n",
    "        if not base_model_name or not epochs or not batch_size or not model_type:\n",
    "            error = \"Base model, number of epochs, batch size, and model type are required.\"\n",
    "            return jsonify({'error': error, 'message': message, 'training_logs': training_logs})\n",
    "\n",
    "        try:\n",
    "            train_dir = os.path.join(dataset_path, 'train')\n",
    "            validation_dir = os.path.join(dataset_path, 'validation')\n",
    "            epochs = int(epochs)\n",
    "            fine_tune_epochs = int(fine_tune_epochs) if fine_tune_epochs else 0\n",
    "            batch_size = int(batch_size)  # Convert batch size to integer\n",
    "            \n",
    "            # Data Generators\n",
    "            img_height, img_width = 224, 224  # Adjust these dimensions as needed\n",
    "            train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2,\n",
    "                                               height_shift_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "            val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "            train_generator = train_datagen.flow_from_directory(train_dir, target_size=(img_height, img_width),\n",
    "                                                                batch_size=batch_size, class_mode='categorical')\n",
    "            val_generator = val_datagen.flow_from_directory(validation_dir, target_size=(img_height, img_width),\n",
    "                                                            batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "            # Model Creation\n",
    "            model = create_model(base_model_name, num_classes=len(train_generator.class_indices))\n",
    "\n",
    "            # Logging callback\n",
    "            class LoggingCallback(tf.keras.callbacks.Callback):\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    training_logs.append({\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"accuracy\": logs.get('accuracy'),\n",
    "                        \"val_accuracy\": logs.get('val_accuracy'),\n",
    "                        \"loss\": logs.get('loss'),\n",
    "                        \"val_loss\": logs.get('val_loss')\n",
    "                    })\n",
    "\n",
    "            # GPU Configuration\n",
    "            gpus = tf.config.list_physical_devices('GPU')\n",
    "            if gpus:\n",
    "                try:\n",
    "                    for gpu in gpus:\n",
    "                        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                    message = \"Training phase will utilize GPU.\"\n",
    "                except RuntimeError as e:\n",
    "                    message = \"Failed to configure GPU. Continuing on CPU. Please be patient!\"\n",
    "            else:\n",
    "                message = \"No GPU found. Training will proceed on CPU.\"\n",
    "            \n",
    "            # Model training\n",
    "            history = model.fit(\n",
    "                train_generator,\n",
    "                validation_data=val_generator,\n",
    "                epochs=epochs,\n",
    "                callbacks=[LoggingCallback()]  \n",
    "            )\n",
    "\n",
    "            # Fine-tuning phase\n",
    "            if fine_tune == 'true' and fine_tune_epochs > 0:\n",
    "                model.trainable = True  # Unfreeze the model for fine-tuning\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "                fine_tune_history = model.fit(\n",
    "                    train_generator,\n",
    "                    validation_data=val_generator,\n",
    "                    epochs=fine_tune_epochs,\n",
    "                    callbacks=[LoggingCallback()]  \n",
    "                )\n",
    "\n",
    "            # Get the best validation accuracy from logs\n",
    "            final_val_accuracy = training_logs[-1]['val_accuracy']\n",
    "            formatted_val_accuracy = f\"{final_val_accuracy * 100:.2f}\"  # Format as percentage\n",
    "\n",
    "            # Get the current date\n",
    "            training_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "            # Save the model with accuracy, date, and model type in the filename\n",
    "            models_folder = 'models'\n",
    "            # Check if the model type is \"pest\" or \"crop\" and set the subfolder accordingly\n",
    "            if model_type == \"pest\":\n",
    "                model_subfolder = os.path.join(models_folder, \"pest\")\n",
    "            elif model_type == \"crop\":\n",
    "                model_subfolder = os.path.join(models_folder, \"crop\")\n",
    "            else:\n",
    "                model_subfolder = models_folder \n",
    "                \n",
    "            os.makedirs(model_subfolder, exist_ok=True)\n",
    "            model_name = f\"{model_type}_{base_model_name}_{len(train_generator.class_indices)}_classes_{formatted_val_accuracy}_acc_{training_date}.keras\"\n",
    "            \n",
    "            # Attempt to save the model and log any errors\n",
    "            try:\n",
    "                model.save(os.path.join(model_subfolder, model_name))\n",
    "                logging.info(f\"Model saved successfully at: {model_name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error saving model: {e}\")\n",
    "\n",
    "            # Prepare model info file path\n",
    "            model_info_path = os.path.join(model_subfolder, f\"{model_type}_{base_model_name}_{len(train_generator.class_indices)}_classes_{formatted_val_accuracy}_acc_{training_date}.txt\")\n",
    "            \n",
    "            # Attempt to create the model info file and log any errors\n",
    "            try:\n",
    "                with open(model_info_path, 'w') as f:\n",
    "                    f.write(f\"Model: {base_model_name}\\n\")\n",
    "                    f.write(f\"Number of Classes: {len(train_generator.class_indices)}\\n\")\n",
    "                    f.write(f\"Classes: {list(train_generator.class_indices.keys())}\\n\")\n",
    "                    f.write(f\"Training Accuracy: {training_logs[-1]['accuracy']:.4f}\\n\")\n",
    "                    f.write(f\"Validation Accuracy: {training_logs[-1]['val_accuracy']:.4f}\\n\")\n",
    "                logging.info(f\"Model info file created successfully at: {model_info_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error creating model info file: {e}\")\n",
    "\n",
    "            message = f\"Model training completed successfully with validation accuracy of {formatted_val_accuracy}%!\"\n",
    "            return jsonify({'error': error, 'message': message, 'training_logs': training_logs})\n",
    "\n",
    "        except Exception as e:\n",
    "            error = f\"An error occurred during training: {str(e)}\"\n",
    "            return jsonify({'error': error, 'message': message, 'training_logs': training_logs})\n",
    "\n",
    "    # Render the training form (for GET requests)\n",
    "    return render_template('train.html', message=message, error=error, training_logs=training_logs)\n",
    "\n",
    "\n",
    "@app.route('/inference', methods=['GET', 'POST'])\n",
    "def inference():\n",
    "    error = None\n",
    "    models = []\n",
    "\n",
    "    # Load available models based on selected type\n",
    "    if request.method == 'POST':\n",
    "        model_type = request.form.get('model_type')  # Get the model type from the form\n",
    "        model_choice = request.form.get('model_choice')  # Get the model choice from the form\n",
    "\n",
    "        # image upload\n",
    "        if 'image' not in request.files or request.files['image'].filename == '':\n",
    "            error = \"No image uploaded. Please upload an image.\"\n",
    "            return render_template('inference.html', models=models, error=error)\n",
    "\n",
    "        # Save the uploaded file\n",
    "        file = request.files['image']\n",
    "        filename = secure_filename(file.filename)\n",
    "        file_path = os.path.join('static/uploads', filename)\n",
    "        file.save(file_path)\n",
    "\n",
    "        # Prepare the image for prediction\n",
    "        img = prepare_image(file_path)\n",
    "\n",
    "        # Load the selected model\n",
    "        model_path = os.path.join('models', model_type, model_choice)\n",
    "        try:\n",
    "            model = tf.keras.models.load_model(model_path)\n",
    "            logging.info(f\"Loaded model: {model_choice}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading model: {e}\")\n",
    "            return f\"Error loading model: {e}\", 500\n",
    "\n",
    "        # Load class labels\n",
    "        try:\n",
    "            class_labels = load_model_info(model_choice, model_type)  # Pass both arguments\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading class labels: {e}\")\n",
    "            return f\"Error loading class labels: {e}\", 500\n",
    "\n",
    "        # Perform prediction\n",
    "        try:\n",
    "            prediction = model.predict(img)\n",
    "            predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "            predicted_label = class_labels[predicted_class]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during prediction: {e}\")\n",
    "            return f\"Error during prediction: {e}\", 500\n",
    "\n",
    "        # Render the result page with the predicted label\n",
    "        return render_template('result.html', label=predicted_label)\n",
    "\n",
    "    # For GET requests, show available models\n",
    "    models = get_available_models('pest') + get_available_models('crop')\n",
    "    return render_template('inference.html', models=models, error=error)\n",
    "    \n",
    "@app.route('/get_models/<model_type>', methods=['GET'])\n",
    "def get_models(model_type):\n",
    "    models = get_available_models(model_type.lower())\n",
    "    return jsonify(models=models)\n",
    "\n",
    "@app.route('/result')\n",
    "def result():\n",
    "    return render_template('result.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=4269)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944414a0-128d-43d0-8fa8-baa2126beffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5852fba-5f8c-41e2-8e27-3554c2c659f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
